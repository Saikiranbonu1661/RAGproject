# ğŸ“š RAG Document QA System

A production-ready Retrieval-Augmented Generation (RAG) system with **two interfaces**: Legacy Streamlit and Modern FastAPI + React full-stack application.

## ğŸŒŸ Features

- **ğŸ“¤ Dynamic Document Upload** - Upload PDFs, DOCX, TXT, MD files on-the-fly
- **ğŸ¤– AI-Powered Q&A** - OpenAI integration for intelligent answers
- **ğŸ” Semantic Search** - Elasticsearch vector storage with BGE embeddings
- **ğŸ’¬ Modern Chat UI** - React-based interface with real-time updates
- **ğŸ”¢ Token Tracking** - Monitor usage with detailed breakdown
- **ğŸ“š Source Attribution** - See which documents were used
- **ğŸ¨ Two UIs** - Choose Streamlit (simple) or FastAPI+React (production)

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      HTTP/REST      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   React     â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚   FastAPI    â”‚
â”‚  Frontend   â”‚                     â”‚   Backend    â”‚
â”‚  (Port 3000)â”‚                     â”‚  (Port 8000) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â”‚
                                           â–¼
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚Elasticsearch â”‚
                                    â”‚  (Port 9200) â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â”‚
                                           â–¼
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚   OpenAI      â”‚
                                    â”‚  gpt-4.1-nano â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
sampleprojects/
â”œâ”€â”€ backend/              # FastAPI backend
â”‚   â””â”€â”€ api.py           # RESTful API
â”œâ”€â”€ frontend/            # React frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/  # UI components
â”‚   â”‚   â”œâ”€â”€ services/    # API integration
â”‚   â”‚   â””â”€â”€ App.js       # Main app
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ src/rag_qa/          # Core RAG system
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ rag_openai.py       # OpenAI integration
â”‚   â”‚   â”œâ”€â”€ es7_retriever.py    # Elasticsearch retrieval
â”‚   â”‚   â””â”€â”€ document_processor.py
â”‚   â””â”€â”€ utils/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yml       # Unified configuration
â”œâ”€â”€ requirements.txt     # All Python dependencies
â”œâ”€â”€ package.json         # Frontend dependencies
â”œâ”€â”€ app.py              # Streamlit UI (legacy)
â””â”€â”€ main.py             # Terminal interface
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- Node.js 16+ and npm
- Docker (for Elasticsearch)

### Installation

```bash
# 1. Install Python dependencies
pip install -r requirements.txt

# 2. Install Frontend dependencies
npm install

# 3. Start Elasticsearch
docker-compose up -d

# 4. Configure API key
# Edit config/config.yml and add your OpenAI API key
```

### Run the Application

**Option 1: Full-Stack Web UI (Recommended)**

```bash
# Terminal 1 - Backend
cd backend
python3 api.py

# Terminal 2 - Frontend
npm start

# Open http://localhost:3000
```

**Option 2: Terminal Interface**

```bash
python3 main.py
```

## ğŸ”§ Configuration

Edit `config/config.yml`:

```yaml
llm:
  use_openai: true
  api_key: "your-openai-api-key"
  model: "gpt-4.1-nano"  # or gpt-4o-mini for cheaper
  params:
    max_tokens: 1000
    temperature: 0

vector_store:
  elasticsearch:
    es_url: "http://localhost:9200"
    index_name: "rag_pdf_chunks_v1"

retrieval:
  top_k: 3  # Number of chunks to retrieve
```

## ğŸ“– Documentation

- **[INSTALL.md](INSTALL.md)** - Quick installation guide
- **[README_FULLSTACK.md](README_FULLSTACK.md)** - Full-stack application guide
- **[FASTAPI_REACT_SETUP.md](FASTAPI_REACT_SETUP.md)** - Detailed setup & troubleshooting

## ğŸ¯ Key Features

- âœ… **Dynamic Document Upload** - Upload files on-the-fly
- âœ… **Advanced Chat Interface** - Modern React UI
- âœ… **Detailed Token Tracking** - Monitor usage
- âœ… **Source Attribution** - Expandable sources
- âœ… **Multi-Session Support** - Per-user isolation
- âœ… **RESTful API** - Full API access
- âœ… **Production Ready** - Scalable architecture

## ğŸ”§ Tech Stack

**Backend:**
- FastAPI - Modern Python web framework
- LangChain - RAG pipeline orchestration
- Elasticsearch - Vector storage & retrieval
- OpenAI - Language model (gpt-4.1-nano, gpt-4o-mini)
- Sentence Transformers - BGE embeddings (768-dim)

**Frontend:**
- React 18 - UI framework
- Axios - HTTP client
- react-dropzone - File uploads
- react-markdown - Markdown rendering

## ğŸ¨ Screenshots

**Modern React UI:**
- Clean sidebar with document management
- Real-time chat interface
- Token usage display
- Source attribution

**Streamlit UI:**
- Simple chat interface
- Pre-indexed documents
- Basic Q&A functionality

## ğŸ“Š API Endpoints

```
POST   /api/session/create        - Create new session
POST   /api/documents/upload      - Upload document
POST   /api/query                 - Ask question
GET    /api/session/{id}          - Get session info
DELETE /api/session/{id}          - Delete session
```

**API Docs:** http://localhost:8000/docs

## ğŸ” How It Works

### 1. Document Ingestion
- Upload PDF/DOCX/TXT/MD files
- Process with UnstructuredFileIOLoader
- Split into chunks (tiktoken-based, 100 tokens)
- Generate 768-dim BGE embeddings
- Store in Elasticsearch

### 2. Query Processing
- User asks a question
- Generate query embedding
- Retrieve top-K similar chunks (cosine similarity)
- Build context from retrieved chunks

### 3. Answer Generation
- Send context + question to OpenAI
- Generate concise answer (max 50 words)
- Track token usage (prompt + completion)
- Display sources used

## ğŸ› Troubleshooting

**Elasticsearch not running:**
```bash
docker-compose up -d
curl http://localhost:9200
```

**Module not found:**
```bash
pip install -r requirements.txt
```

**Port already in use:**
```bash
lsof -ti:8000 | xargs kill -9  # Backend
lsof -ti:3000 | xargs kill -9  # Frontend
```

**node_modules being committed:**
- Already added to `.gitignore`
- Run: `git rm -r --cached node_modules`

## ğŸš€ Production Deployment

**Backend:**
```bash
gunicorn -w 4 -k uvicorn.workers.UvicornWorker backend.api:app
```

**Frontend:**
```bash
cd frontend && npm run build
# Serve build/ with nginx
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch
3. Commit changes
4. Push to branch
5. Open pull request

## ğŸ“„ License

MIT License - See LICENSE file

## ğŸ™ Credits

- **OpenAI** - Language models
- **Elasticsearch** - Vector storage
- **LangChain** - RAG framework
- **React** - Frontend framework
- **FastAPI** - Backend framework

---

**Made with â¤ï¸ using FastAPI + React + OpenAI**
